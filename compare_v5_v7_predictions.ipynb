{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V5 vs V7 Final - Ensemble Prediction Comparison\n",
        "\n",
        "Detailed comparison of predictions on test well 15_9-F-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V5 samples: 1138\n",
            "V7 samples: 1138\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "sys.path.append('config')\n",
        "from optimization_metrics import custom_sand_weighted_f1\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load predictions\n",
        "v5 = pd.read_csv('v5_full_progressive/predictions/15_9-F-4/ensemble_predictions.csv')\n",
        "v7 = pd.read_csv('v7_final_output/predictions/15_9-F-4/ensemble_predictions.csv')\n",
        "\n",
        "print(f\"V5 samples: {len(v5)}\")\n",
        "print(f\"V7 samples: {len(v7)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Test Well Performance\n",
        "\n",
        "**Important Note:** \n",
        "- Models were optimized for **custom_sand** metric (reservoir-focused weighted F1)\n",
        "- Raw accuracy is reference only\n",
        "- Fair comparison = custom_sand score on test well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TEST WELL PERFORMANCE (15_9-F-4)\n",
            "======================================================================\n",
            "\n",
            "1. RAW ACCURACY (reference only):\n",
            "   V5: 0.4895 (48.95%)\n",
            "   V7: 0.4534 (45.34%)\n",
            "   Difference: -3.60% ‚Üí V5 wins\n",
            "\n",
            "2. CUSTOM SAND F1 (PRIMARY - what V7 optimized for!):\n",
            "   V5: 0.4771\n",
            "   V7: 0.4411\n",
            "   Difference: -3.59% ‚Üí V5 wins\n",
            "\n",
            "3. F1 WEIGHTED (SECONDARY - balanced metric):\n",
            "   V5: 0.4509\n",
            "   V7: 0.4222\n",
            "   Difference: -2.87% ‚Üí V5 wins\n",
            "\n",
            "======================================================================\n",
            "WINNER ON TEST WELL:\n",
            "üèÜ V5 Full Progressive (custom_sand: 0.4771 vs 0.4411)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate all metrics on test well\n",
        "y_true = v5['Facies_True'].values\n",
        "y_pred_v5 = v5['Facies_Predicted'].values\n",
        "y_pred_v7 = v7['Facies_Predicted'].values\n",
        "\n",
        "# 1. Raw accuracy (reference)\n",
        "acc_v5 = (y_pred_v5 == y_true).mean()\n",
        "acc_v7 = (y_pred_v7 == y_true).mean()\n",
        "\n",
        "# 2. Custom Sand F1 (PRIMARY METRIC - what models were optimized for!)\n",
        "custom_sand_v5 = custom_sand_weighted_f1(y_true, y_pred_v5)\n",
        "custom_sand_v7 = custom_sand_weighted_f1(y_true, y_pred_v7)\n",
        "\n",
        "# 3. F1 Weighted (SECONDARY METRIC)\n",
        "f1_v5 = f1_score(y_true, y_pred_v5, average='weighted', zero_division=0)\n",
        "f1_v7 = f1_score(y_true, y_pred_v7, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TEST WELL PERFORMANCE (15_9-F-4)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. RAW ACCURACY (reference only):\")\n",
        "print(f\"   V5: {acc_v5:.4f} ({acc_v5*100:.2f}%)\")\n",
        "print(f\"   V7: {acc_v7:.4f} ({acc_v7*100:.2f}%)\")\n",
        "print(f\"   Difference: {(acc_v7-acc_v5)*100:+.2f}% ‚Üí {'V7 wins' if acc_v7 > acc_v5 else 'V5 wins'}\")\n",
        "\n",
        "print(\"\\n2. CUSTOM SAND F1 (PRIMARY - what V7 optimized for!):\")\n",
        "print(f\"   V5: {custom_sand_v5:.4f}\")\n",
        "print(f\"   V7: {custom_sand_v7:.4f}\")\n",
        "print(f\"   Difference: {(custom_sand_v7-custom_sand_v5)*100:+.2f}% ‚Üí {'V7 wins' if custom_sand_v7 > custom_sand_v5 else 'V5 wins'}\")\n",
        "\n",
        "print(\"\\n3. F1 WEIGHTED (SECONDARY - balanced metric):\")\n",
        "print(f\"   V5: {f1_v5:.4f}\")\n",
        "print(f\"   V7: {f1_v7:.4f}\")\n",
        "print(f\"   Difference: {(f1_v7-f1_v5)*100:+.2f}% ‚Üí {'V7 wins' if f1_v7 > f1_v5 else 'V5 wins'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"WINNER ON TEST WELL:\")\n",
        "if custom_sand_v7 > custom_sand_v5:\n",
        "    print(f\"üèÜ V7 Final (custom_sand: {custom_sand_v7:.4f} vs {custom_sand_v5:.4f})\")\n",
        "else:\n",
        "    print(f\"üèÜ V5 Full Progressive (custom_sand: {custom_sand_v5:.4f} vs {custom_sand_v7:.4f})\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training vs Test Performance Gap\n",
        "\n",
        "**This shows generalization ability**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUSTOM SAND METRIC (Primary Optimization Target):\n",
            "----------------------------------------------------------------------\n",
            "Model           Train CV     Test Score   Gap          Generalization\n",
            "----------------------------------------------------------------------\n",
            "V5              0.3917      0.4771      +0.0854      Good\n",
            "V7 Final        0.4902      0.4411      -0.0491      Overfit\n",
            "\n",
            "======================================================================\n",
            "WINNER: V5 by 0.0359\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Training CV scores (from model rankings)\n",
        "v5_best_cv = 0.3917  # FC_13 custom_sand\n",
        "v7_best_cv = 0.4902  # V7_Rank_04 custom_sand\n",
        "\n",
        "print(\"CUSTOM SAND METRIC (Primary Optimization Target):\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Model':<15} {'Train CV':<12} {'Test Score':<12} {'Gap':<12} {'Generalization'}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "v5_gap = custom_sand_v5 - v5_best_cv\n",
        "v7_gap = custom_sand_v7 - v7_best_cv\n",
        "\n",
        "print(f\"{'V5':<15} {v5_best_cv:.4f}      {custom_sand_v5:.4f}      {v5_gap:+.4f}      {'Good' if v5_gap >= 0 else 'Overfit'}\")\n",
        "print(f\"{'V7 Final':<15} {v7_best_cv:.4f}      {custom_sand_v7:.4f}      {v7_gap:+.4f}      {'Good' if v7_gap >= 0 else 'Overfit'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if custom_sand_v7 > custom_sand_v5:\n",
        "    print(f\"WINNER: V7 Final by {(custom_sand_v7-custom_sand_v5):.4f}\")\n",
        "else:\n",
        "    print(f\"WINNER: V5 by {(custom_sand_v5-custom_sand_v7):.4f}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Uncertainty Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uncertainty Metrics:\n",
            "  Entropy (lower=better):   V5=1.1126, V7=1.1116 ‚Üí V7 wins\n",
            "  Agreement (higher=better): V5=0.7852, V7=0.8511 ‚Üí V7 wins\n",
            "  Margin (higher=better):    V5=0.3616, V7=0.3344 ‚Üí V5 wins\n"
          ]
        }
      ],
      "source": [
        "entropy_v5 = v5['Uncertainty_Entropy'].mean()\n",
        "entropy_v7 = v7['Uncertainty_Entropy'].mean()\n",
        "agreement_v5 = v5['Uncertainty_Agreement'].mean()\n",
        "agreement_v7 = v7['Uncertainty_Agreement'].mean()\n",
        "margin_v5 = v5['Uncertainty_Margin'].mean()\n",
        "margin_v7 = v7['Uncertainty_Margin'].mean()\n",
        "\n",
        "print(\"Uncertainty Metrics:\")\n",
        "print(f\"  Entropy (lower=better):   V5={entropy_v5:.4f}, V7={entropy_v7:.4f} ‚Üí {'V7 wins' if entropy_v7 < entropy_v5 else 'V5 wins'}\")\n",
        "print(f\"  Agreement (higher=better): V5={agreement_v5:.4f}, V7={agreement_v7:.4f} ‚Üí {'V7 wins' if agreement_v7 > agreement_v5 else 'V5 wins'}\")\n",
        "print(f\"  Margin (higher=better):    V5={margin_v5:.4f}, V7={margin_v7:.4f} ‚Üí {'V7 wins' if margin_v7 > margin_v5 else 'V5 wins'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agreement Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction Agreement:\n",
            "  Both correct: 428/1138 (37.6%)\n",
            "  Both wrong: 493/1138 (43.3%)\n",
            "  Only V5 correct: 129/1138 (11.3%)\n",
            "  Only V7 correct: 88/1138 (7.7%)\n",
            "\n",
            "Net improvement: -41 samples\n"
          ]
        }
      ],
      "source": [
        "both_correct = ((v5['Facies_Predicted'] == v5['Facies_True']) & (v7['Facies_Predicted'] == v7['Facies_True'])).sum()\n",
        "both_wrong = ((v5['Facies_Predicted'] != v5['Facies_True']) & (v7['Facies_Predicted'] != v7['Facies_True'])).sum()\n",
        "v5_only = ((v5['Facies_Predicted'] == v5['Facies_True']) & (v7['Facies_Predicted'] != v7['Facies_True'])).sum()\n",
        "v7_only = ((v5['Facies_Predicted'] != v5['Facies_True']) & (v7['Facies_Predicted'] == v7['Facies_True'])).sum()\n",
        "\n",
        "print(\"Prediction Agreement:\")\n",
        "print(f\"  Both correct: {both_correct}/{len(v5)} ({both_correct/len(v5)*100:.1f}%)\")\n",
        "print(f\"  Both wrong: {both_wrong}/{len(v5)} ({both_wrong/len(v5)*100:.1f}%)\")\n",
        "print(f\"  Only V5 correct: {v5_only}/{len(v5)} ({v5_only/len(v5)*100:.1f}%)\")\n",
        "print(f\"  Only V7 correct: {v7_only}/{len(v7)} ({v7_only/len(v7)*100:.1f}%)\")\n",
        "print(f\"\\nNet improvement: {v7_only - v5_only:+d} samples\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
